{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import ast\n",
    "\n",
    "import geopandas\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "\n",
    "from word2number import w2n\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk import stem\n",
    "stemmer = stem.snowball.EnglishStemmer()\n",
    "import string\n",
    "\n",
    "english_words = words.words()\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import pytextrank\n",
    "tr = pytextrank.TextRank()\n",
    "nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n",
    "from sklearn.decomposition import PCA\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import cm\n",
    "\n",
    "from matplotlib import colors\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file = 'china_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_data = pd.read_csv(raw_data_file)\n",
    "\n",
    "##modity dtyeps\n",
    "china_data['first_sentence_list'] = [ast.literal_eval(i) for i in china_data['first_sentence_list']]\n",
    "china_data['first_sentence_pos_list'] = [ast.literal_eval(i) for i in china_data['first_sentence_pos_list']]\n",
    "china_data['first_sentence_useful_list'] = [ast.literal_eval(i) for i in china_data['first_sentence_useful_list']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First clause reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence_list = []\n",
    "first_sentence_pos_list = []\n",
    "null_cnt = china_data['first_sentence_list'].isnull().sum()\n",
    "\n",
    "for i, fs, fsp in zip(china_data['translated_company_desc'].values,\n",
    "                       china_data['first_sentence_list'].values,\n",
    "                       china_data['first_sentence_pos_list'].values):\n",
    "    if True:\n",
    "        try:\n",
    "            ##get all clauses from the first sentence\n",
    "            first_period = ' '.join(\n",
    "        [i.text for i in nlp(i)][:[(i.pos_=='PUNCT') and (i.text=='.') for i in nlp(i)].index(True)]\n",
    "    ).lower().replace('electricity business', 'online commerce')  #fix translation might need deletion\n",
    "            first_period = first_period.replace('electric business', 'online commerce') # fix translation\n",
    "            first_period = first_period.replace('electronic business', 'online commerce')\n",
    "            first_period = first_period.replace('us - pupil','cosmetic contact lenses')\n",
    "        except: \n",
    "            #some texts have no period\n",
    "            print('no period detected')\n",
    "            first_period = i.lower().replace('electricity business', 'online commerce') # fix translation\n",
    "            first_period = first_period.replace('electric business', 'online commerce')\n",
    "            first_period = first_period.replace('electronic business', 'online commerce')\n",
    "            first_period = first_period.replace('us - pupil','cosmetic contact lenses')\n",
    "            \n",
    "        texts = first_period.split(',') # first clause only \n",
    "        list_useful = []\n",
    "        list_useful_pos = []\n",
    "        for text in texts:\n",
    "            pos = [i.pos_ for i in nlp(text)]\n",
    "            text = [i.text for i in nlp(text)]\n",
    "            # skip clauses with no verb or aux \n",
    "            if ('VERB' not in pos) and ('AUX' not in pos):\n",
    "                continue\n",
    "            list_useful.append(text)\n",
    "            list_useful_pos.append(pos)\n",
    "        try:\n",
    "            subset = list_useful[0]\n",
    "            subset_pos = list_useful_pos[0]\n",
    "            \n",
    "            min_words = 15 ##first clause needs at least 15 words. If not, refer to the second, the third, .. if available\n",
    "            recur = 1\n",
    "            \n",
    "            while len(subset)< min_words and len(list_useful)>= recur+1:\n",
    "                subset = list(itertools.chain.from_iterable(list_useful[:recur+1]))\n",
    "                subset_pos = list(itertools.chain.from_iterable(list_useful_pos[:recur+1]))\n",
    "                recur+=1\n",
    "                print('extending...')\n",
    "                print(subset)\n",
    "        except:\n",
    "            # extract keywords from the whole text if no period detected.\n",
    "            subset = list(np.array([i.text for i in nlp(first_period)])[\n",
    "    [i.pos_ in ['NOUN','ADJ','PROPN'] for i in nlp(first_period)]])\n",
    "            subset_pos = list(np.array([i.pos_ for i in nlp(first_period)])[\n",
    "    [i.pos_ in ['NOUN','ADJ','PROPN'] for i in nlp(first_period)]]) \n",
    "    \n",
    "    else:\n",
    "        print('done already')\n",
    "        subset = fs#.astype(list)\n",
    "        subset_pos = fsp#.astype(list)\n",
    "        \n",
    "    print('finally..')\n",
    "    print(subset)\n",
    "    print(subset_pos)\n",
    "    first_sentence_list.append(subset)\n",
    "    first_sentence_pos_list.append(subset_pos)\n",
    "    \n",
    "china_data['first_sentence_list'] = first_sentence_list\n",
    "china_data['first_sentence_pos_list'] = first_sentence_pos_list  \n",
    "print(china_data['first_sentence_list'].isnull().sum())\n",
    "\n",
    "china_data['first_sentence_list'] = china_data['first_sentence_list'].astype(object)\n",
    "china_data['first_sentence_pos_list']= china_data['first_sentence_pos_list'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sentence_useful_list = []\n",
    "for i, j,u in zip(china_data.first_sentence_pos_list,\n",
    "                  china_data.first_sentence_list,\n",
    "                  china_data.first_sentence_useful_list):\n",
    "    if True:\n",
    "        ### filter company name by the position of first verb/aux\n",
    "        try:\n",
    "            index1 = i.index('VERB')\n",
    "        except:\n",
    "            index1 = 0\n",
    "        try:\n",
    "            index2 = i.index('AUX')\n",
    "        except:\n",
    "            index2 = 0\n",
    "        index_position = min(index1,index2)\n",
    "        ## keyword\n",
    "        useful_list = [y for x, y in zip(i[index_position+1:],\n",
    "                       j[index_position+1:]) if x in ['NOUN','ADJ','PROPN']] \n",
    "        \n",
    "        ## might not needed\n",
    "        if 'diagnostic' in j and 'diagnostic' not in useful_list:\n",
    "            #print(j)\n",
    "            useful_list.append('diagnostic') #correct\n",
    "        ##--------------------    \n",
    "        print(index_position, useful_list)\n",
    "    else:\n",
    "        useful_list = u\n",
    "        \n",
    "    first_sentence_useful_list.append(useful_list)  \n",
    "    \n",
    "print(china_data['first_sentence_useful_list'].isnull().sum())\n",
    "china_data['first_sentence_useful_list'] = first_sentence_useful_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sector Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_list = []\n",
    "cat_list = []\n",
    "\n",
    "custom_stopwords = ['invest','investor','china',\"investment\", \"capital\",\n",
    "                   'business','market','chinese', 'investments','financing','company',\n",
    "                   'development', 'us','dollar','yuan','billion','million','round',\n",
    "                   'rmb','investors','dollars','investor','valuation','share','stock',\n",
    "                   'acquisition',\"stocks\",'shares','angel','venture','millions','billions','first',\n",
    "                   'second','third','industry','company','products',\n",
    "                   'services','brand','developer','developers','platform','2nd','results','announcement',\n",
    "                    'provider','providers',\n",
    "                    'developer','developers',\n",
    "                   'object','total','tens','number','brands', 'enterprise',\n",
    "                    'price', 'equity', 'partnership', 'people','creation','companies','production',\n",
    "                   'more','issue','product','delisting','group','strategic', 'cooperation', 'agreement',\n",
    "                   'subsidiary','world','today','name','results', 'announcement','dry','hereinafter',\n",
    "                   'chain','enterprise','innovative','innovation','enterprises',\n",
    "                   'technology',\"tech\",'development','prospectus', 'cornerstone', 'amount',\n",
    "                   'fiscal', 'year', 'earnings','users','user','design','technologies','technological','high'\n",
    "                   ,'ai','pupil','degree','electronic',\n",
    "                   'international','global','institutions'] \n",
    "\n",
    "\n",
    "target_sector = ['manufacturer','automobile','semiconductor','hardware', \n",
    "                 'buildings','estate',\n",
    "                 'financial', 'payment',\n",
    "                 'information',\"internet\", 'software',\n",
    "                 \"healthcare\",\"medical\",\"pharmaceutical\",'biotech',\n",
    "                 \"commerce\",\"retail\",\n",
    "                 'education',\n",
    "                 'game', \n",
    "                 'food',\n",
    "                 'drink'\n",
    "                ]\n",
    "\n",
    "\n",
    "for corpus,top, cat in zip(china_data['first_sentence_useful_list'].values,\n",
    "                  china_data['top_one'].values,\n",
    "                  china_data['cat_list'].values):\n",
    "    \n",
    "    RUN_ALL = True\n",
    "    if RUN_ALL:\n",
    "        ## filter stopwords\n",
    "        corpus = [i for i in corpus if i not in custom_stopwords]\n",
    "        print(corpus)\n",
    "        if len(corpus)>=1:\n",
    "            list_top_k = []\n",
    "            list_term = []\n",
    "            list_simi = []\n",
    "            for term in target_sector:\n",
    "                for top_k in corpus:\n",
    "                    top_k = top_k.lower()\n",
    "                    if top_k in custom_stopwords:\n",
    "                        continue\n",
    "                    try:\n",
    "                        ## similarity = 1/cosine_distance\n",
    "                        list_simi.append(word2vec_model.similarity(top_k,term.lower()))\n",
    "                        list_top_k.append(top_k)\n",
    "                        list_term.append(term)\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "            tmp = pd.DataFrame({'top_k':list_top_k,\n",
    "                                'term':list_term,\n",
    "                                'simi':list_simi})  \n",
    "            if len(tmp)!=0:\n",
    "                cat = tmp.groupby('term').mean().sort_values('simi').index[-1]  #mean, median\n",
    "                top_k = tmp[tmp['term']==cat].sort_values('simi').top_k.values[-1]\n",
    "                top_k_list.append(top_k+' ')\n",
    "                \n",
    "                \n",
    "                ###Might not needed\n",
    "                general_list = ['supplier','operators','robot',\"provider\",'developer'] #manufacturer\n",
    "                \n",
    "                if cat in general_list: #  too general\n",
    "                    print('detected')\n",
    "                    #selected_cat = sorted(tmp.groupby('term').mean().sort_values('simi').index[-2:])\n",
    "                    #selected_cat = tmp.groupby('term').mean().sort_values('simi').index[-2:]\n",
    "                    #cat = '-'.join(selected_cat)\n",
    "                    cat = tmp.groupby('term').mean().sort_values('simi').index[-2]\n",
    "                    if cat in general_list:\n",
    "                        cat = tmp.groupby('term').mean().sort_values('simi').index[-1]\n",
    "                        \n",
    "                if cat=='hardware':\n",
    "                    if 'hardware' in corpus:\n",
    "                        cat = 'hardware'\n",
    "                    else: \n",
    "                        cat = tmp.groupby('term').mean().sort_values('simi').index[-2]\n",
    "                        \n",
    "                ##=================================\n",
    "                cat_list.append(cat+' ')\n",
    "                print(top_k, cat)\n",
    "                \n",
    "            else:\n",
    "                top_k_list.append(' ')\n",
    "                cat_list.append(' ')\n",
    "                print('None')\n",
    "        else:\n",
    "            top_k_list.append(' ')\n",
    "            cat_list.append(' ')\n",
    "            print('None')\n",
    "        \n",
    "china_data['top_one'] = top_k_list\n",
    "china_data['cat_list'] = cat_list   \n",
    "print(china_data['top_one'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Sector Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_cols = ['Electronics',\n",
    "              'RE',\n",
    "               'IT',\n",
    "              'Biotech',\n",
    "             'Education'\n",
    "             ]\n",
    "\n",
    "\n",
    "proxy_sectors = [['hardware'], #semiconductor\n",
    "                [ 'buildings','estate'],\n",
    "                ['information',\"internet\", 'software'],\n",
    "               [ \"healthcare\",\"medical\",\"pharmaceutical\"],\n",
    "               [ 'education']]\n",
    "                \n",
    "    \n",
    "dict_sector_cat = {}\n",
    "chosen = [item for sublist in proxy_sectors for item in sublist]\n",
    "\n",
    "for i, j in zip(proxy_cols,proxy_sectors):\n",
    "    for ji in j:\n",
    "        dict_sector_cat [ji] = i  \n",
    "\n",
    "key_words = china_data['top_one']\n",
    "sectors = china_data['cat_list']\n",
    "\n",
    "vec_list = []\n",
    "key_word_ = []\n",
    "key_sector = []\n",
    "for i,j in zip(key_words,sectors):\n",
    "    try:\n",
    "        vec_list.append(word2vec_model.get_vector(i.strip()))\n",
    "        key_word_.append(i.strip())\n",
    "        key_sector.append(j.strip())\n",
    "    except:\n",
    "        #print(i)\n",
    "        pass\n",
    "print(len(key_word_))\n",
    "print(len(key_sector))\n",
    "\n",
    "tmp_df = pd.DataFrame(vec_list)\n",
    "tmp_df['sector'] = key_sector\n",
    "tmp_df['word'] = key_word_\n",
    "\n",
    "print(tmp_df.shape)\n",
    "tmp_df = tmp_df.drop_duplicates()\n",
    "\n",
    "tmp_df = tmp_df[[i in chosen for i in tmp_df[\"sector\"]]]\n",
    "tmp_df['sector'] = tmp_df['sector'].apply(lambda x:dict_sector_cat[x])    \n",
    "tmp_df_subset = tmp_df.groupby('sector').head(25)\n",
    "\n",
    "X = np.array(tmp_df_subset.iloc[:,:-2].values)\n",
    "nkmeans= len(tmp_df_subset['sector'].unique())\n",
    "kmeans = KMeans(n_clusters=nkmeans, random_state=0).fit(X)\n",
    "kmeans.labels_\n",
    "kmeans.cluster_centers_\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "ax.scatter(X_pca[:,0],X_pca[:,1], c= le.fit_transform(\n",
    "    tmp_df_subset['sector']), cmap='rainbow',s = 1)\n",
    "\n",
    "colors=cm.Dark2(np.linspace(0,1,len(tmp_df_subset['sector'].unique())))\n",
    "dict_ ={i:j for i, j in zip(tmp_df_subset['sector'].unique(),\n",
    "                            colors)}\n",
    "for i, txt in enumerate(tmp_df_subset['sector'].values):\n",
    "    ax.annotate(txt, (X_pca[:,0][i], X_pca[:,1][i]),color=dict_[txt],fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cld = [i for i in china_data['first_sentence_useful_list'].values]\n",
    "word_cld = [item for sublist in word_cld for item in sublist]\n",
    "text = ' '.join(word_cld)\n",
    "\n",
    "viridis = cm.get_cmap('viridis')\n",
    "wordcloud = WordCloud(max_font_size=25,\n",
    "                     background_color=\"white\", \n",
    "               stopwords=custom_stopwords, contour_width=3,\n",
    "                     contour_color='white'\n",
    "                     ).generate(text)\n",
    "plt.figure(figsize = (16,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_df = pd.read_excel('NFFAI_per_sector/fixed_investment202005.xls').iloc[1:,:]\n",
    "official_df.columns = [i.replace('年','').replace('月','') for i in official_df.iloc[0,:].values]\n",
    "official_df = official_df.set_index('指标')\n",
    "official_df.columns = [i[:4]+'-'+i[4:].zfill(2) for i in official_df.columns]\n",
    "official_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "official_cols = ['医药制造业固定资产投资额累计增长(%)',\n",
    "                '制造业固定资产投资额累计增长(%)',\n",
    "                '信息传输、软件和信息技术服务业固定资产投资额累计增长(%)',\n",
    "                '房地产业固定资产投资额累计增长(%)',\n",
    "                '计算机、通信和其他电子设备制造业固定资产投资额累计增长(%)',\n",
    "                 '金融业固定资产投资额累计增长(%)',\n",
    "                #'住宿和餐饮业固定资产投资额累计增长(%)',\n",
    "                 '教育固定资产投资额累计增长(%)',\n",
    "                '批发和零售业固定资产投资额累计增长(%)',\n",
    "                '汽车制造业固定资产投资额累计增长(%)',\n",
    "                '食品制造业固定资产投资额累计增长(%)',\n",
    "                '酒、饮料和精制茶制造业固定资产投资额累计增长(%)',\n",
    "                #'文化艺术业固定资产投资额累计增长(%)',\n",
    "                 '文化、体育和娱乐业固定资产投资额累计增长(%)',\n",
    "                 #'交通运输、仓储和邮政业固定资产投资额累计增长(%)'\n",
    "                 #'科学研究和技术服务业固定资产投资额累计增长(%)',\n",
    "                 #'电气机械和器材制造业固定资产投资额累计增长(%)',\n",
    "                 #'化学纤维制造业固定资产投资额累计增长(%)'\n",
    "                ] #'纺织服装、服饰业固定资产投资额累计增长(%)'\n",
    "\n",
    "\n",
    "proxy_cols = ['Medical',\n",
    "              'Manufacturing',\n",
    "               'IT',\n",
    "              'RE',\n",
    "              #'estate',\n",
    "              'Electronics',\n",
    "             'financial',\n",
    "             #'catering',\n",
    "             'education',\n",
    "             'retail&wholesale',\n",
    "             'automotive',\n",
    "             'food',\n",
    "             'drink',\n",
    "             #'culture',\n",
    "              'entertainment',\n",
    "              #'logistics'\n",
    "              #'science research'\n",
    "             #'machinery',\n",
    "             ] #\"clothing\" 'material'\n",
    "\n",
    "\n",
    "proxy_sectors = [['pharmaceutical','medical','healthcare'],\n",
    "                 ['manufacturer','automobile','semiconductor','hardware'],#'food','drink'], #machinery\n",
    "                 ['information',\"internet\",\"software\"],\n",
    "                 ['buildings','estate'],\n",
    "                 ['semiconductor','hardware'],\n",
    "                 ['financial','payment'],\n",
    "                ['education'],\n",
    "                ['commerce','food','drink'],\n",
    "                 ['automobile'],\n",
    "                 ['food'],\n",
    "                 ['drink'],\n",
    "               [ 'game'],\n",
    "               # ['logistics'],\n",
    "                ]\n",
    "\n",
    "if len(proxy_sectors) == len(proxy_cols) and len(proxy_cols) == len(official_cols):\n",
    "    list_df = []\n",
    "    for official_col, proxy_col, proxy_sector in zip(\n",
    "official_cols,\n",
    "    proxy_cols,\n",
    "    proxy_sectors\n",
    "): \n",
    "        print(proxy_sector)\n",
    "        official_sub = official_df.ix[official_col].to_frame()\n",
    "        official_sub.columns = ['official']\n",
    "        tmp = china_data[[i.strip() in proxy_sector\n",
    "                  for i in china_data['cat_list']]].groupby(\n",
    "'date').count()[\"title_url\"].to_frame()\n",
    "        tmp = pd.DataFrame({'date':pd.date_range(tmp.index[0],\n",
    "              tmp.index[-1])}).set_index('date').join(\n",
    "        tmp).fillna(0)\n",
    "        tmp = tmp.rolling(7*4).sum().dropna()\n",
    "        tmp = tmp.reset_index()\n",
    "        tmp.date = list(map(lambda x:str(x)[:7],tmp.date))\n",
    "        result = tmp.groupby('date').mean().pct_change(12).dropna().join(\n",
    "        official_sub\n",
    ").dropna()\n",
    "        result['sector'] = [proxy_col]*len(result)\n",
    "        list_df.append(result.reset_index(drop=False))\n",
    "else:\n",
    "    print('ERROR!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = iter([plt.cm.tab20(i) for i in range(len(pd.concat(list_df,axis=0)['date'].unique()))])\n",
    "result_df = pd.concat(list_df,axis=0)\n",
    "result_df[['title_url','official']] = result_df[['title_url','official']].astype(float)\n",
    "#result_df = result_df.groupby('sector').mean().reset_index()\n",
    "result_df.plot.scatter(x='title_url',y = 'official', figsize=(16,8))\n",
    "months = pd.concat(list_df,axis=0)['date'].unique()\n",
    "#months = ['2020-03']\n",
    "colors = [plt.cm.Dark2(i) for i in range(len(months))]\n",
    "patch_list=[]\n",
    "for mo, color in zip(months,colors): \n",
    "    tmp_df = result_df[result_df['date']==mo]\n",
    "    patch = mpatches.Patch(color=color, label=mo)\n",
    "    patch_list.append(patch)\n",
    "    for label, x, y in zip(tmp_df['sector'], tmp_df[\"title_url\"].astype(float), \n",
    "                       tmp_df['official'].astype(float)):\n",
    "        plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc=color, alpha=1),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "        \n",
    "plt.legend(handles=patch_list)\n",
    "plt.ylabel('China Official Fixed Income Investment Monthly YoY by sector.')\n",
    "plt.xlabel('Alternative source on PE deals Monthly YoY by Sector.')\n",
    "print('corr: '+ str(result_df.corr().iloc[0,-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need modification here. \n",
    "## by sector\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "cv_results = cross_validate(lm, result_df['title_url'].values.reshape(len(result_df),-1), \n",
    "                            result_df['official'], cv=5,scoring = 'neg_mean_squared_error'\n",
    "                           ) #neg_mean_absolute_error  #neg_mean_squared_error\n",
    "print(cv_results['test_score'])\n",
    "print()\n",
    "print(np.mean(cv_results['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#market_expectation \n",
    "expected = pd.DataFrame({'date': ['2020-02', '2020-03', '2020-04','2020-05'],\n",
    "'expected': [-1.8,-15,-9.5,-5.9],\n",
    "'true': [-24.5,-16.1, -10.3,-6.3]}).set_index('date')\n",
    "\n",
    "print(expected.corr())\n",
    "#1. http://economy.caixin.com/2020-03-16/101528938.html\n",
    "#2. https://finance.sina.com.cn/money/bond/market/2020-04-16/doc-iircuyvh8165813.shtml\n",
    "#3.https://cn.wsj.com/articles/%E4%B8%AD%E5%9B%BD1-4%E6%9C%88%E4%B8%8D%E5%90%AB%E5%86%9C%E6%88%B7%E5%9B%BA%E5%AE%9A%E8%B5%84%E4%BA%A7%E6%8A%95%E8%B5%84%E5%90%8C%E6%AF%94%E4%B8%8B%E9%99%8D10-3%EF%BC%8C%E4%B8%8D%E5%8F%8A%E9%A2%84%E6%9C%9F-11589508909\n",
    "#4.https://cn.reuters.com/article/china-may-industry-consumption-poll-0612-idCNKBS23J1MU\n",
    "expected.plot.scatter(x='expected',y = 'true',s = 50)\n",
    "\n",
    "mean_squared_error(expected['expected'].values, \n",
    "                   expected.true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
